"""
è·¨å¹³å°é¤å…åŒ¹é… â€” åŸºäºåŸå§‹ match_restaurants.py
ä¿ç•™: thefuzz æ¨¡ç³ŠåŒ¹é…ã€å¤šç­–ç•¥åŒ¹é…ã€è¿é”åº—åç¼€å¤„ç†ã€ä¸€è‡´æ€§è¯„åˆ†
"""
import math
import re
from typing import List, Dict

from models import DianpingRestaurant, XiaohongshuPost, MatchedRestaurant

# thefuzz æ˜¯å¯é€‰ä¾èµ–ï¼Œæ²¡è£…æ—¶å›é€€åˆ°ç®€å•åŒ¹é…
try:
    from thefuzz import fuzz
    HAS_FUZZ = True
except ImportError:
    HAS_FUZZ = False
    print("ğŸ’¡ æç¤º: å®‰è£… thefuzz å¯è·å¾—æ›´å¥½çš„åŒ¹é…æ•ˆæœ: pip3 install thefuzz")


# è¿é”åº—å¸¸è§åç¼€ï¼ˆæ¥è‡ªåŸå§‹ match_restaurants.pyï¼‰
CHAIN_SUFFIXES = re.compile(
    r'[ï¼ˆ(].{0,15}[)ï¼‰]|'
    r'(é™å®‰|å¾æ±‡|æµ¦ä¸œ|æœé˜³|æµ·æ·€|å—å±±|ç¦ç”°|å¤©æ²³|æ­¦ä¾¯|é”¦æ±Ÿ|å—æ²¹|åå¼º|ç§‘æŠ€å›­)'
    r'(åº—|åˆ†åº—|æ——èˆ°åº—|æ€»åº—)?$'
)


def normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–é¤å…åï¼šå»é™¤åˆ†åº—åç¼€ã€ç©ºæ ¼ã€ç‰¹æ®Šç¬¦å·"""
    name = name.strip()
    name = CHAIN_SUFFIXES.sub('', name)
    name = re.sub(r'[\sÂ·ãƒ»\-â€”]+', '', name)
    return name


def calculate_similarity(dp_name: str, xhs_name: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªåº—åçš„ç›¸ä¼¼åº¦ï¼ˆ0~1ï¼‰
    ä½¿ç”¨å¤šç­–ç•¥åŒ¹é…ï¼ˆæ¥è‡ªåŸå§‹ match_restaurants.py._calculate_similarityï¼‰
    """
    dp_norm = normalize_name(dp_name)
    xhs_norm = normalize_name(xhs_name)

    if not dp_norm or not xhs_norm:
        return 0.0

    # ç­–ç•¥1: å®Œå…¨åŒ¹é…
    if dp_norm == xhs_norm:
        return 1.0

    if HAS_FUZZ:
        # ç­–ç•¥2: ç²¾ç¡®æ¯”ç‡
        exact_score = fuzz.ratio(dp_norm, xhs_norm) / 100
        # ç­–ç•¥3: éƒ¨åˆ†åŒ¹é…
        partial_score = fuzz.partial_ratio(dp_norm, xhs_norm) / 100
        # ç­–ç•¥4: Token æ’åº
        token_score = fuzz.token_sort_ratio(dp_norm, xhs_norm) / 100
    else:
        # ç®€å• Jaccard ç›¸ä¼¼åº¦ä½œä¸ºå›é€€
        s1, s2 = set(dp_norm), set(xhs_norm)
        exact_score = len(s1 & s2) / len(s1 | s2) if (s1 | s2) else 0
        partial_score = 0
        token_score = 0

    # ç­–ç•¥5: åŒ…å«å…³ç³»
    containment_score = 0.0
    if dp_norm in xhs_norm or xhs_norm in dp_norm:
        shorter = min(len(dp_norm), len(xhs_norm))
        longer = max(len(dp_norm), len(xhs_norm))
        containment_score = shorter / longer if longer > 0 else 0.0

    # å–æœ€ä¼˜ç­–ç•¥
    return max(
        exact_score,
        partial_score * 0.90,
        token_score * 0.85,
        containment_score * 0.88,
    )


def normalize_engagement(xhs_post: XiaohongshuPost) -> float:
    """
    å°†å°çº¢ä¹¦äº’åŠ¨é‡å½’ä¸€åŒ–åˆ° 0-5 è¯„åˆ†ï¼ˆæ¥è‡ªåŸå§‹ match_restaurants.pyï¼‰
    ä½¿ç”¨å¯¹æ•°å½’ä¸€åŒ–é¿å…æç«¯å€¼å½±å“
    """
    engagement = (
        xhs_post.likes * 1.0 +
        xhs_post.saves * 2.0 +
        xhs_post.comments * 1.5
    )

    if engagement <= 0:
        return 0.0

    # log1p(5000) â‰ˆ 8.52 ä½œä¸º"æ»¡åˆ†"å‚è€ƒç‚¹
    normalized = math.log1p(engagement) / math.log1p(5000) * 5
    return max(0.0, min(5.0, normalized))


def calculate_consistency(
    dp_rating: float,
    xhs_engagement_normalized: float,
    xhs_sentiment: float,
) -> float:
    """
    è®¡ç®—ä¸¤å¹³å°ä¸€è‡´æ€§è¯„åˆ† 0~1ï¼ˆæ¥è‡ªåŸå§‹ match_restaurants.pyï¼‰
    """
    dp_rating = max(0.0, min(5.0, dp_rating))
    xhs_engagement_normalized = max(0.0, min(5.0, xhs_engagement_normalized))
    xhs_sentiment = max(-1.0, min(1.0, xhs_sentiment))

    # è¯„åˆ†ç›¸å…³æ€§
    rating_diff = abs(dp_rating - xhs_engagement_normalized)
    rating_correlation = max(0.0, 1.0 - (rating_diff / 2.5))

    # æƒ…æ„Ÿä¸€è‡´æ€§
    sentiment_normalized = (xhs_sentiment + 1) / 2  # -1~1 â†’ 0~1
    sentiment_alignment = sentiment_normalized

    return max(0.0, min(1.0, rating_correlation * 0.6 + sentiment_alignment * 0.4))


def match_and_score(
    dp_restaurants: List[DianpingRestaurant],
    xhs_posts: List[XiaohongshuPost],
    similarity_threshold: float = 0.55,
) -> List[MatchedRestaurant]:
    """
    è·¨å¹³å°åŒ¹é…å¹¶è®¡ç®—ä¸€è‡´æ€§è¯„åˆ†

    Args:
        dp_restaurants: å¤§ä¼—ç‚¹è¯„æ•°æ®
        xhs_posts: å°çº¢ä¹¦æ•°æ®
        similarity_threshold: åŒ¹é…é˜ˆå€¼

    Returns:
        åŒ¹é…ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ä¸€è‡´æ€§æ’åº
    """
    matches = []
    used_xhs = set()

    for dp in dp_restaurants:
        best_idx, best_score = None, 0

        for idx, xhs in enumerate(xhs_posts):
            if idx in used_xhs:
                continue
            score = calculate_similarity(dp.name, xhs.restaurant_name)
            if score > best_score and score >= similarity_threshold:
                best_score = score
                best_idx = idx

        if best_idx is not None:
            xhs = xhs_posts[best_idx]
            used_xhs.add(best_idx)

            # è®¡ç®—ä¸€è‡´æ€§
            xhs_engagement_norm = normalize_engagement(xhs)
            consistency = calculate_consistency(
                dp.rating, xhs_engagement_norm, xhs.sentiment_score
            )

            matches.append(MatchedRestaurant(
                name=dp.name,
                dianping_data=dp,
                xhs_data=xhs,
                similarity_score=best_score,
                consistency_score=consistency,
            ))

    return matches
