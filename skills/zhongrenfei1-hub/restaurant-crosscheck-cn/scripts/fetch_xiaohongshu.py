"""
å°çº¢ä¹¦æŠ“å– â€” sync Playwright ç‰ˆæœ¬
åŸºäºåŸå§‹ fetch_xiaohongshu_real.py é‡æ„ï¼Œä¿®å¤ asyncâ†’syncï¼Œæ”¹è¿›åº—åæå–
"""
import os
import re
import time
from typing import List
from urllib.parse import quote

from playwright.sync_api import sync_playwright

from config import (
    XHS_SESSION, BROWSER_ARGS,
    PAGE_LOAD_WAIT, SCROLL_WAIT, MAX_XHS_NOTES,
    XHS_CARD_SELECTORS,
    POSITIVE_WORDS, NEGATIVE_WORDS, NEGATION_WORDS,
    SHOP_SUFFIXES, EXCLUDE_KEYWORDS,
)
from models import XiaohongshuPost


def fetch_xiaohongshu(location: str, cuisine: str) -> List[XiaohongshuPost]:
    """
    ä»å°çº¢ä¹¦æŠ“å–é¤å…ç›¸å…³ç¬”è®°

    Args:
        location: åœ°ç†ä½ç½®ï¼Œå¦‚ "æ·±åœ³å—å±±åŒº"
        cuisine: èœç³»/ç±»å‹ï¼Œå¦‚ "ç²¤èœ"

    Returns:
        XiaohongshuPost åˆ—è¡¨ï¼ˆå·²æŒ‰é¤å…åå»é‡åˆå¹¶ï¼‰
    """
    raw_posts = []

    try:
        with sync_playwright() as p:
            os.makedirs(XHS_SESSION, exist_ok=True)
            ctx = p.chromium.launch_persistent_context(
                XHS_SESSION, headless=False, args=BROWSER_ARGS,
            )
            page = ctx.new_page()

            keyword = f"{location} {cuisine} æ¨è"
            url = (
                f"https://www.xiaohongshu.com/search_result?"
                f"keyword={quote(keyword)}&source=web_search_result_notes"
            )
            print(f"  ğŸ”— å°çº¢ä¹¦: {url}")
            page.goto(url, timeout=30000, wait_until="domcontentloaded")
            time.sleep(PAGE_LOAD_WAIT + 1)

            # å¦‚æœè¢«é‡å®šå‘ï¼Œç”¨æœç´¢æ¡†
            if "/search_result" not in page.url:
                print("  âš ï¸ è¢«é‡å®šå‘ï¼Œå°è¯•æœç´¢æ¡†...")
                page.goto("https://www.xiaohongshu.com", timeout=30000)
                time.sleep(3)
                box = page.query_selector(
                    'input[placeholder*="æœç´¢"], #search-input, input[type="text"]'
                )
                if box:
                    box.click()
                    box.fill(keyword)
                    time.sleep(0.5)
                    page.keyboard.press("Enter")
                    time.sleep(PAGE_LOAD_WAIT + 1)

            # æ»šåŠ¨åŠ è½½æ›´å¤š
            for _ in range(3):
                page.evaluate("window.scrollBy(0, 600)")
                time.sleep(SCROLL_WAIT)

            # æŠ“å–ç¬”è®°å¡ç‰‡
            cards = []
            for sel in XHS_CARD_SELECTORS:
                cards = page.query_selector_all(sel)
                if cards:
                    break
            print(f"  ğŸ“Š æ‰¾åˆ° {len(cards)} ç¯‡ç¬”è®°")

            if not cards:
                page.screenshot(path=os.path.expanduser("~/Downloads/xhs_debug.png"))
                print("  âš ï¸ æœªæ‰¾åˆ°ç¬”è®°ï¼Œæˆªå›¾: ~/Downloads/xhs_debug.png")

            # ä»ç¬”è®°æ ‡é¢˜æå–åº—å
            seen_names = set()
            for card in cards[:MAX_XHS_NOTES]:
                try:
                    title_el = card.query_selector('.title, [class*="title"], span, .desc')
                    title = title_el.inner_text().strip() if title_el else card.inner_text().strip()
                    if not title:
                        continue

                    # æå–ç‚¹èµæ•°
                    likes = _parse_likes(card)

                    # æå–åº—å
                    names = extract_restaurant_names(title)
                    sentiment = analyze_sentiment(title)
                    keywords = extract_keywords(title)

                    for name in names:
                        if name not in seen_names:
                            seen_names.add(name)
                            raw_posts.append(XiaohongshuPost(
                                restaurant_name=name,
                                likes=likes,
                                saves=0,
                                comments=0,
                                sentiment_score=sentiment,
                                keywords=keywords,
                                mention_count=1,
                            ))

                except Exception:
                    continue

            ctx.close()

    except Exception as e:
        print(f"  âŒ å°çº¢ä¹¦å‡ºé”™: {e}")

    # åˆå¹¶åŒåé¤å…
    return _merge_posts(raw_posts)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  è¾…åŠ©å‡½æ•°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _parse_likes(card) -> int:
    """ä»å¡ç‰‡å…ƒç´ ä¸­æå–ç‚¹èµæ•°"""
    like_el = card.query_selector('[class*="like"], [class*="count"]')
    if not like_el:
        return 0
    m = re.search(r'(\d+\.?\d*)\s*(ä¸‡|k)?', like_el.inner_text())
    if not m:
        return 0
    likes = float(m.group(1))
    if m.group(2) in ('ä¸‡', 'k'):
        likes *= 10000
    return int(likes)


def extract_restaurant_names(text: str) -> List[str]:
    """
    ä»ç¬”è®°æ ‡é¢˜ä¸­æå–é¤å…åï¼ˆåŸºäºåŸå§‹ fetch_xiaohongshu_real.py çš„é€»è¾‘ï¼Œå¤§å¹…å¢å¼ºï¼‰
    """
    names = []

    # 1. ä¹¦åå· / å¼•å·å†…çš„åç§°ï¼ˆå°çº¢ä¹¦æœ€å¸¸è§çš„æ ¼å¼ï¼‰
    for pattern in [r'ã€Œ(.+?)ã€', r'ã€(.+?)ã€', r'ã€Š(.+?)ã€‹', r'ã€(.+?)ã€‘',
                    r'\[(.+?)\]', r'"(.+?)"', r'"(.+?)"']:
        names.extend(re.findall(pattern, text))

    # 2. æ¢åº—/æ‰“å¡æ¨¡å¼: "æ¢åº—XXX"ã€"æ‰“å¡XXX"ï¼ˆæ¥è‡ªåŸå§‹ _extract_restaurant_nameï¼‰
    explore_match = re.search(
        r'(?:æ¢åº—|æ‰“å¡|å®‰åˆ©|æ¨è|ç§è‰)[ï¼š:\s]*(.{2,20}?)(?:[ï¼!,ï¼Œã€‚\s]|å¤ª|è¶…|çœŸ|$)',
        text
    )
    if explore_match:
        names.append(explore_match.group(1).strip())

    # 3. åŒ…å«é¤é¥®åç¼€çš„è¯ç»„
    suffix_pattern = '|'.join(re.escape(s) for s in SHOP_SUFFIXES)
    names.extend(re.findall(
        rf'([\u4e00-\u9fff\w]{{2,12}}(?:{suffix_pattern}))',
        text
    ))

    # 4. åˆ†éš”ç¬¦åˆ†å‰²çš„ç‰‡æ®µï¼ˆå¦‚ "åº—åAï½œåº—åBï½œæ¢åº—"ï¼‰
    if any(sep in text for sep in ['|', 'ï½œ', '/', 'Â·']):
        for part in re.split(r'[|ï½œ/Â·]', text):
            part = part.strip()
            if 2 <= len(part) <= 15 and not any(k in part for k in EXCLUDE_KEYWORDS):
                names.append(part)

    # å»é‡ä¿åºï¼Œè¿‡æ»¤å¤ªçŸ­çš„
    seen = set()
    result = []
    for n in names:
        n = n.strip()
        # å»é™¤ emoji
        n = re.sub(r'[ğŸ’•ğŸ”¥âœ¨ğŸŒŸğŸ“â—ğŸœğŸ²ğŸ¥˜ğŸ±ğŸ£ğŸ»â˜•ğŸ‰ğŸ‘ğŸ’¯]+', '', n).strip()
        if len(n) >= 2 and n not in seen:
            seen.add(n)
            result.append(n)
    return result


def analyze_sentiment(text: str) -> float:
    """
    æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºåŸå§‹ fetch_xiaohongshu_real.py çš„é€»è¾‘ï¼Œå¸¦å¦å®šè¯å¤„ç†ï¼‰
    è¿”å› -1 ~ 1
    """
    pos_count, neg_count = 0, 0

    for word in POSITIVE_WORDS:
        idx = text.find(word)
        while idx >= 0:
            context = text[max(0, idx - 2):idx]
            if any(context.endswith(n) for n in NEGATION_WORDS):
                neg_count += 1
            else:
                pos_count += 1
            idx = text.find(word, idx + len(word))

    for word in NEGATIVE_WORDS:
        idx = text.find(word)
        while idx >= 0:
            context = text[max(0, idx - 2):idx]
            if any(context.endswith(n) for n in NEGATION_WORDS):
                pos_count += 1  # åŒé‡å¦å®š = æ­£é¢
            else:
                neg_count += 1
            idx = text.find(word, idx + len(word))

    total = pos_count + neg_count
    if total == 0:
        return 0.0
    return (pos_count - neg_count) / total


def extract_keywords(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
    keyword_list = [
        'å¥½åƒ', 'ç¾å‘³', 'ç¯å¢ƒ', 'æœåŠ¡', 'æ€§ä»·æ¯”', 'æ–°é²œ', 'æ­£å®—', 'å€¼å¾—',
        'æ¨è', 'å®è—', 'æ’é˜Ÿ', 'å›å¤´å®¢', 'æƒŠè‰³',
    ]
    return [kw for kw in keyword_list if kw in text][:5]


def _merge_posts(posts: List[XiaohongshuPost]) -> List[XiaohongshuPost]:
    """åˆå¹¶åŒåé¤å…çš„å¸–å­"""
    merged = {}
    for p in posts:
        name = p.restaurant_name
        if name in merged:
            m = merged[name]
            m.mention_count += 1
            m.likes += p.likes
            # å–å„ç¯‡æƒ…æ„Ÿçš„å¹³å‡å€¼
            m.sentiment_score = (m.sentiment_score * (m.mention_count - 1) + p.sentiment_score) / m.mention_count
            # åˆå¹¶å…³é”®è¯
            for kw in p.keywords:
                if kw not in m.keywords:
                    m.keywords.append(kw)
        else:
            merged[name] = XiaohongshuPost(
                restaurant_name=name,
                likes=p.likes, saves=p.saves, comments=p.comments,
                sentiment_score=p.sentiment_score,
                keywords=list(p.keywords),
                mention_count=1,
            )
    return list(merged.values())
