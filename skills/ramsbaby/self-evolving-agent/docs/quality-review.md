# Self-Evolving Agent 제안 품질 비평 리포트

**작성일:** 2026-02-17  
**작성자:** 기술 비평가 서브에이전트  
**버전:** v2.0 → v3.0 전환 기록

---

## 1. 핵심 진단: v2.0의 무엇이 문제였는가

### 1.1 "분석이 아닌 단어 카운팅" 문제

v2.0 analyze-behavior.sh가 실제로 한 일:

```
"다시" → 25건 → 불만 신호로 분류
"계속" → 20건 → 불만 신호로 분류  
"확인" → 4건 → 불만 신호로 분류
```

**실제 맥락 (샘플):**
- "다시 확인해봐" → 정상 요청
- "계속 진행해줘" → 정상 요청
- "확인중" → 에이전트가 작업 중임을 알리는 표현

총 64건 중 실제 불만은 **0건** (v3.0 기준). 이건 분석이 아니라 노이즈 생성기.

### 1.2 치명적 버그: 필드명 오류

```python
# v2.0 코드 (틀림)
if c.get('type') == 'tool_use':   # ← 존재하지 않는 필드명!

# 실제 jsonl 구조
if c.get('type') == 'toolCall':   # ← 올바른 필드명 (camelCase)
```

결과: **tool 분석 전체가 dead code**. 7273회 exec 호출을 0회로 집계. 이 버그로 인해 도구 재시도 분석, 연속 실패 분석 등이 모두 작동 안 함.

### 1.3 오탐 문제: violations 분석

v2.0 방법:
```python
# 에이전트 응답 전체 텍스트에서 grep
hit_count = len(re.findall(r'git (?:pull|push|fetch)', all_assistant_text))
# → 13회 "위반" 감지
```

실제 내용: 에이전트가 대화에서 "git pull --rebase는 금지"라고 **규칙을 설명한 것**을 위반으로 카운트. 

v3.0 방법:
```python
# exec 도구에 실제 전달된 명령어에서만 탐지
search_text = all_exec_cmds  # exec 명령에서만
```

결과: 0건 (오탐 완전 제거).

### 1.4 제안 품질 문제: "연결이 없는 제안"

v2.0 generate-proposal.sh 제안 #4:

```
근거: 반복 요청 패턴: "확인해줘"(25회), "진행해"(14회), "진행해줘"(13회)
제안: 자주 요청되는 작업 자동화 템플릿 추가
```

**문제:** "확인해줘"는 불특정 다목적 표현. 25회 반복이 어떤 *특정 작업*의 자동화를 의미하는지 근거 없음. 이 제안은 모든 에이전트에게 보편적으로 적용할 수 있는 빈말.

---

## 2. v3.0에서 잡아낸 실제 신호들

### 2.1 exec 연속 재시도: 진짜 문제

```
exec 도구를 3회 이상 연속 재시도한 세션: 221개
최대 연속 재시도: 119회 (같은 도구 연속 호출)
총 재시도 이벤트: 405건
```

이게 왜 유의미한가:
- 119회 연속 같은 도구 = "에이전트가 루프에 갇혔다"
- 221개 세션 = 일회성 사고가 아닌 구조적 패턴
- `process` 최대 100회 연속 = process(action=poll) 무한 루프

이걸 v2.0은 전혀 감지 못했다. v3.0이 잡아낸 첫 번째 유의미한 신호.

### 2.2 반복 에러: 미수정 버그의 직접 증거

```
heartbeat-cron.log: 동일 에러 18회 반복
에러 유형: "error: Invalid numeric literal at line N, column N"
```

이게 왜 유의미한가:
- 한 번 = 일시적 오류, 18번 반복 = 버그 미수정
- 타임스탬프 정규화 후에도 동일 → 진짜 같은 버그
- 크론이 실행될 때마다 실패 = 매 실행 비용 낭비

v2.0도 에러 카운트는 했지만 "반복 여부"를 판단하지 못했다.

### 2.3 세션 과부하: 토큰 낭비 패턴

```
컴팩션 5회 이상 세션: 3개
최대 컴팩션: 20회 (1개 세션)
세션당 평균 메시지: 359개
```

컴팩션 20회 = 컨텍스트가 20번 압축됨 = 초기 컨텍스트의 상당 부분 손실. 이런 세션은 서브에이전트 분리가 훨씬 효율적이라는 근거.

---

## 3. "와 이거 진짜 유용하다" 조건 달성 여부

### 3.1 조건 체크

| 조건 | v2.0 | v3.0 |
|------|-------|-------|
| **구체적** (모호한 "더 잘해라" 금지) | ❌ "자동화 템플릿 추가" (무엇을?) | ✅ "exec 3회 실패 시 중단 후 보고" |
| **근거 있음** (어떤 데이터에서?) | ❌ "확인해줘 25회" (불만 아님) | ✅ "exec 119회 연속, 221개 세션" |
| **즉시 적용 가능** (AGENTS.md 복붙) | ❌ 추상적 설명 | ✅ 마크다운 섹션 형태로 제공 |
| **오탐 없음** | ❌ 13건 위반 (실제 0건) | ✅ 0건 (exec 기반 탐지) |
| **새로운 인사이트** | ❌ 알려진 규칙 재확인 | ✅ 미발견 패턴 (119회 루프) |

### 3.2 before/after diff 품질

v2.0:
```
Before: (관련 규칙 없음)
After: ## 🔁 반복 요청 감지 프로토콜
       사용자가 같은 내용을 2회 이상 요청하면...
```
→ Before가 틀림. AGENTS.md에는 이미 "During Conversation" 섹션에 관련 내용 있음.

v3.0:
```
Before: 연속 `exec` 재시도 시 규칙 없음 (무한 루프 가능)
After: ## ⚡ exec 연속 재시도 방지
       같은 exec를 3회 이상 재시도하기 전에:
       1. 첫 번째 실패 시 에러 메시지를 사용자에게 보고
       ...
```
→ Before가 실제 현재 상태. After가 즉시 AGENTS.md에 붙여넣을 수 있는 수준.

---

## 4. 추가 개선 가능한 것들 (v4.0 방향)

### 4.1 실현 가능한 것들 (bash + python, 외부 API 없이)

#### A. 사용자 메시지 길이 급증 패턴
```python
# 사용자 메시지가 갑자기 길어지면 = 불만 누적 신호
# 예: 평소 50자 → 갑자기 300자 = "왜 이렇게 안 되는지 설명"
user_len_changes = [len(msgs[i]) / len(msgs[i-1]) for i in range(1, len(msgs))]
sudden_increase = [c for c in user_len_changes if c > 3.0]  # 3배 이상 증가
```

#### B. 대화 주제 급변 감지 (삼천포 신호)
```python
# 연속 메시지에서 공통 키워드 비율이 낮으면 = 주제 이탈
# 단어 bag-of-words 유사도로 계산 (외부 API 없이)
from collections import Counter
def topic_overlap(text1, text2):
    w1 = set(text1.split()) 
    w2 = set(text2.split())
    return len(w1 & w2) / len(w1 | w2) if w1 | w2 else 1.0
```

#### C. 크론 실행 시간 이상치
```python
# lastDurationMs가 평소보다 5배 이상이면 = 타임아웃 직전
# 이미 cron jobs.json에 lastDurationMs 있음
avg_duration = mean([j['state']['lastDurationMs'] for j in jobs])
slow_jobs = [j for j in jobs if j['state']['lastDurationMs'] > avg_duration * 5]
```

### 4.2 실현 불가능한 것들 (솔직하게)

- **semantic similarity (임베딩)**: 외부 API 필요, 로컬 모델 없이는 불가
- "에이전트가 삼천포 갔는가" 판단: 자연어 이해가 필요, grep/regex로 한계
- 사용자 만족도: 직접 측정 불가, 간접 신호만 추론 가능

---

## 5. UX 개선 평가

### 5.1 승인/거부 이모지 UX (v3.0에서 구현)

```
| ✅ | 전체 승인 |
| 1️⃣ ~ 5️⃣ | 해당 번호만 승인 |
| ❌ | 전체 거부 (이유 댓글) |
| 🔄 | 수정 요청 |
```

**평가:** 실용적. Discord에서 이모지 반응은 마찰이 최소화. 다만 현재 approve-proposal.sh가 이모지 반응을 파싱하는 기능이 없음 → 향후 구현 필요.

### 5.2 거부 피드백 수집

현재: `rejected-proposals.json`에 ID만 저장.  
개선 필요: 거부 이유 텍스트를 저장하고 다음 분석에서 complaint_patterns에 반영.

---

## 6. 요약: 변경 내용

| 구분 | v2.0 | v3.0 |
|------|-------|-------|
| 핵심 버그 | `tool_use` 필드명 오류 (dead code) | `toolCall`로 수정 |
| 불만 패턴 | 64건 (오탐 대부분) | 0건 (실제 불만만) |
| 도구 분석 | 없음 | 재시도 405건, 최대 119회 연속 |
| violations | 13건 (응답 텍스트 grep) | 0건 (exec 명령 기반) |
| 반복 에러 | 단순 카운트 | 같은 에러 N회 = 미수정 버그 |
| 세션 건강도 | 없음 | 컴팩션 기반 분석 |
| 제안 품질 | 추상적, 근거 약함 | 즉시 AGENTS.md 적용 가능 |
| 승인 UX | 텍스트 명령만 | 이모지 반응 + 텍스트 병행 |

---

## 7. 솔직한 평가

v3.0이 v2.0보다 훨씬 낫지만, 여전히 한계가 있다:

1. **연속 재시도 ≠ 반드시 실패**: `exec` 119회 연속이 "프로세스 모니터링" (정상)인지 "무한 루프" (비정상)인지 맥락 없이 판단 불가. threshold 조정 필요.

2. **세션당 MAX 50개 제한**: 964개 세션 중 50개만 분석. 나머지 914개에서 다른 패턴이 있을 수 있음.

3. **영어 로그 한정**: violations 패턴이 영어 명령어 중심. 한국어 지시어 기반 패턴 확장 필요.

그래도 핵심 버그 수정 + 유의미한 신호 발굴만으로도 제안 품질은 크게 향상됨.
